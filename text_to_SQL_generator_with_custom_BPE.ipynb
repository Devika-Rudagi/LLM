{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "r2vvE8Y1WtDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "batch_size = 64\n",
        "block_size = 64        # SQL queries are short, 64 is plenty\n",
        "max_iters = 3000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3   # Slightly higher LR for smaller models\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 100\n",
        "n_embd = 128           # Reduced dim since vocab is small\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.2\n",
        "vocab_size = 1000      # Target vocabulary size (Custom!)\n",
        "\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mM5052mpWsPn",
        "outputId": "864e5309-ce0a-404d-a6c1-70b3613dad20"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7b86c83758f0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# create SQL Data Data\n"
      ],
      "metadata": {
        "id": "-183S10pBX7Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kGb1m9K9ApuP",
        "outputId": "86e34647-ac26-43dc-f1d8-91b8a1446d74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data generated.\n",
            "Q\n",
            "u\n",
            "e\n",
            "s\n",
            "t\n",
            "i\n",
            "o\n",
            "n\n",
            ":\n",
            " \n",
            "F\n",
            "i\n",
            "n\n",
            "d\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            " \n",
            "t\n",
            "i\n",
            "t\n",
            "l\n",
            "e\n",
            " \n",
            "f\n",
            "o\n",
            "r\n",
            " \n",
            "p\n",
            "r\n",
            "o\n",
            "d\n",
            "u\n",
            "c\n",
            "t\n",
            "s\n",
            " \n",
            "w\n",
            "i\n",
            "t\n",
            "h\n",
            " \n",
            "i\n",
            "d\n",
            " \n",
            "2\n",
            "3\n",
            ".\n",
            "\n",
            "\n",
            "S\n",
            "Q\n",
            "L\n",
            ":\n",
            " \n",
            "S\n",
            "E\n",
            "L\n",
            "E\n",
            "C\n",
            "T\n",
            " \n",
            "t\n",
            "i\n",
            "t\n",
            "l\n",
            "e\n",
            " \n",
            "F\n",
            "R\n",
            "O\n",
            "M\n",
            " \n",
            "p\n",
            "r\n",
            "o\n",
            "d\n",
            "u\n",
            "c\n",
            "t\n",
            "s\n",
            " \n",
            "W\n",
            "H\n",
            "E\n",
            "R\n",
            "E\n",
            " \n",
            "i\n",
            "d\n",
            " \n",
            "=\n",
            " \n",
            "2\n",
            "3\n",
            ";\n",
            "\n",
            "\n",
            "<\n",
            "E\n",
            "N\n",
            "D\n",
            ">\n",
            "\n",
            "\n",
            "Q\n",
            "u\n",
            "e\n",
            "s\n",
            "t\n",
            "i\n",
            "o\n",
            "n\n",
            ":\n",
            " \n",
            "F\n",
            "i\n",
            "n\n",
            "d\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            " \n",
            "t\n",
            "i\n",
            "t\n",
            "l\n",
            "e\n",
            " \n",
            "f\n",
            "o\n",
            "r\n",
            " \n",
            "p\n",
            "r\n",
            "o\n",
            "d\n",
            "u\n",
            "c\n",
            "t\n",
            "s\n",
            " \n",
            "w\n",
            "i\n",
            "t\n",
            "h\n",
            " \n",
            "i\n",
            "d\n",
            " \n",
            "2\n",
            "1\n",
            ".\n",
            "\n",
            "\n",
            "S\n",
            "Q\n",
            "L\n",
            ":\n",
            " \n",
            "S\n",
            "E\n",
            "L\n",
            "E\n",
            "C\n",
            "T\n",
            " \n",
            "t\n",
            "i\n",
            "t\n",
            "l\n",
            "e\n",
            " \n",
            "F\n",
            "R\n",
            "O\n",
            "M\n",
            " \n",
            "p\n",
            "r\n",
            "o\n",
            "d\n",
            "u\n",
            "c\n",
            "t\n",
            "s\n",
            " \n",
            "W\n",
            "H\n",
            "E\n",
            "R\n",
            "E\n",
            " \n",
            "i\n",
            "d\n",
            " \n",
            "=\n",
            " \n",
            "2\n",
            "1\n",
            ";\n",
            "\n",
            "\n",
            "<\n",
            "E\n",
            "N\n",
            "D\n",
            ">\n",
            "\n",
            "\n",
            "Q\n",
            "u\n",
            "e\n",
            "s\n",
            "t\n",
            "i\n",
            "o\n",
            "n\n",
            ":\n",
            " \n",
            "L\n",
            "i\n",
            "s\n",
            "t\n",
            " \n",
            "a\n",
            "l\n",
            "l\n",
            " \n",
            "u\n",
            "s\n",
            "e\n",
            "r\n",
            "s\n",
            " \n",
            "w\n",
            "h\n",
            "e\n",
            "r\n",
            "e\n",
            " \n",
            "i\n",
            "d\n",
            " \n",
            "i\n",
            "s\n",
            " \n",
            "9\n",
            "2\n",
            ".\n",
            "\n",
            "\n",
            "S\n",
            "Q\n",
            "L\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def generate_sql_data(num_samples= 10000):\n",
        "    # 1. Define simple schema and templates\n",
        "    tables = ['users', 'products', 'orders', 'employees']\n",
        "    columns = {\n",
        "        'users': ['id', 'name', 'email', 'signup_date'],\n",
        "        'products': ['id', 'title', 'price', 'stock_count'],\n",
        "        'orders': ['id', 'user_id', 'order_date', 'total_amount'],\n",
        "        'employees': ['id', 'first_name', 'department', 'salary']\n",
        "    }\n",
        "\n",
        "    # 2. Define patterns (English -> SQL)\n",
        "    templates = [\n",
        "        (\n",
        "            \"Show me the {col} from {tab}.\",\n",
        "            \"SELECT {col} FROM {tab};\"\n",
        "        ),\n",
        "        (\n",
        "            \"List all {tab} where {col} is {val}.\",\n",
        "            \"SELECT * FROM {tab} WHERE {col} = '{val}';\"\n",
        "        ),\n",
        "        (\n",
        "            \"Find the {col} for {tab} with id {val}.\",\n",
        "            \"SELECT {col} FROM {tab} WHERE id = {val};\"\n",
        "        ),\n",
        "        (\n",
        "            \"Count the number of {tab}.\",\n",
        "            \"SELECT count(*) FROM {tab};\"\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # 3. Generate 10,000 examples\n",
        "    data = \"\"\n",
        "    for _ in range(10000):\n",
        "        # Pick a random table and column\n",
        "        tab = random.choice(tables)\n",
        "        col = random.choice(columns[tab])\n",
        "        val = random.randint(1, 100)\n",
        "\n",
        "        # Pick a random question template\n",
        "        tmpl_q, tmpl_s = random.choice(templates)\n",
        "\n",
        "        # Fill in the blanks\n",
        "        q = tmpl_q.format(tab=tab, col=col, val=val)\n",
        "        s = tmpl_s.format(tab=tab, col=col, val=val)\n",
        "\n",
        "        # Format: Question -> SQL -> <END>\n",
        "        data += f\"Question: {q}\\nSQL: {s}\\n<END>\\n\"\n",
        "    return \"\\n\".join(data)\n",
        "\n",
        "# 4. Save to file\n",
        "raw_text = generate_sql_data()\n",
        "with open(\"sql_dataset.txt\", \"w\") as f:\n",
        "    f.write(raw_text)\n",
        "\n",
        "print(\"Data generated.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the Hugging Face tokenizers library to train a custom BPE tokenizer specifically for your SQL dataset."
      ],
      "metadata": {
        "id": "lepYnaDBCOt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dowJwxITCQsM",
        "outputId": "3fdf8d63-47e2-4fcd-82c2-309a25b69e95"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.22.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the tokenizer - Custom BPE tokenizer\n",
        "Instead of building stoi and itos manually, we feed our sql_data.txt into the BPE trainer. It will find the most common patterns (like \"SELECT\", \"count\", \"id\") and assign them unique IDs."
      ],
      "metadata": {
        "id": "kk-brCwHCXdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "print(\"Training Custom BPE Tokenizer...\")\n",
        "\n",
        "# 1. Initialize an empty BPE tokenizer\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace() # Split by whitespace first\n",
        "\n",
        "# 2. Configure the trainer\n",
        "# vocab_size=1000 is plenty for our tiny SQL vocabulary.\n",
        "# In GPT-4, this is usually 100,000+.\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[END]\"], vocab_size=1000)\n",
        "\n",
        "# 3. Train on your file\n",
        "files = [\"sql_data.txt\"]\n",
        "tokenizer.train(files, trainer)\n",
        "\n",
        "# 4. Save it\n",
        "tokenizer.save(\"custom_sql_tokenizer.json\")\n",
        "\n",
        "# --- TEST IT ---\n",
        "encoded = tokenizer.encode(\"SELECT * FROM users\")\n",
        "print(f\"Tokens: {encoded.tokens}\")\n",
        "print(f\"IDs:    {encoded.ids}\")\n",
        "# You should see something like: ['SELECT', '*', 'FROM', 'users']\n",
        "# and IDs like [12, 5, 14, 25] (Single integers for whole words!)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNNtPyi6CUJz",
        "outputId": "a5055062-f824-4ec9-836e-714afc6b4f36"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Custom BPE Tokenizer...\n",
            "Tokens: ['SELECT', '*', 'FROM', 'users']\n",
            "IDs:    [79, 6, 77, 99]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare data"
      ],
      "metadata": {
        "id": "MfwZDqwlXc4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the whole dataset\n",
        "full_ids = tokenizer.encode(raw_text).ids\n",
        "data_tensor = torch.tensor(full_ids, dtype=torch.long)\n",
        "\n",
        "# Train/Val split\n",
        "n = int(0.9 * len(data_tensor))\n",
        "train_data = data_tensor[:n]\n",
        "val_data = data_tensor[n:]\n",
        "\n",
        "# Update vocab_size to exactly what was trained\n",
        "actual_vocab_size = tokenizer.get_vocab_size()"
      ],
      "metadata": {
        "id": "JrmGeBdWXW9y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definitions (same as miniGPT)"
      ],
      "metadata": {
        "id": "zgAHbe7gXpko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        # Using PyTorch's optimized MultiheadAttention\n",
        "        self.sa = nn.MultiheadAttention(embed_dim=n_embd, num_heads=n_head,\n",
        "                                        dropout=dropout, batch_first=True)\n",
        "        self.ffwd = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        # Creates a (T, T) matrix where future positions are -inf\n",
        "        #Generate the mask manually to avoid version errors\n",
        "        attn_mask = nn.Transformer.generate_square_subsequent_mask(T).to(device)\n",
        "\n",
        "        x_norm = self.ln1(x)\n",
        "        attn_out, _ = self.sa(x_norm, x_norm, x_norm, attn_mask = attn_mask,\n",
        "                              need_weights=False)\n",
        "        x = x + attn_out\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Note: We use actual_vocab_size here\n",
        "        self.token_embedding_table = nn.Embedding(actual_vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, actual_vocab_size)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "Z2OBkUaDXgap"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "zmxskzhHY78f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in m.parameters())/1e6:.2f}M\")\n",
        "print(\"Starting training...\")\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step {iter}: Train Loss {losses['train']:.4f}, Val Loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J5ZgLj1XgXx",
        "outputId": "c1197391-75b0-4eac-edf2-e97337fac8f7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 0.87M\n",
            "Starting training...\n",
            "Step 0: Train Loss 5.6254, Val Loss 5.6262\n",
            "Step 500: Train Loss 0.1760, Val Loss 0.1720\n",
            "Step 1000: Train Loss 0.1673, Val Loss 0.1661\n",
            "Step 1500: Train Loss 0.1651, Val Loss 0.1643\n",
            "Step 2000: Train Loss 0.1575, Val Loss 0.1582\n",
            "Step 2500: Train Loss 0.1532, Val Loss 0.1532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test"
      ],
      "metadata": {
        "id": "R_hiy4kYZAfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sql_custom(question):\n",
        "    prompt = f\"Question: {question}\\nSQL:\"\n",
        "    input_ids = tokenizer.encode(prompt).ids\n",
        "    context = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
        "    output_ids = m.generate(context, max_new_tokens=30)[0].tolist()\n",
        "    output_text = tokenizer.decode(output_ids)\n",
        "\n",
        "    try:\n",
        "        start_idx = output_text.find(\"SQL:\") + len(\"SQL:\")\n",
        "        generated = output_text[start_idx:]\n",
        "        if \"<END>\" in generated:\n",
        "            generated = generated.split(\"<END>\")[0]\n",
        "        return generated.strip()\n",
        "    except:\n",
        "        return output_text\n",
        "\n",
        "test_qs = [\n",
        "    \"Show me the price from products.\",\n",
        "    \"List all users where name is Alice.\",\n",
        "    \"Count the number of employees.\",\n",
        "    \"Find the department for employees with id 10.\"\n",
        "]\n",
        "\n",
        "for q in test_qs:\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"SQL: {generate_sql_custom(q)}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qvm01HziXgVc",
        "outputId": "d7c7e4cb-0f8b-4052-d70b-265806765f2e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Show me the price from products.\n",
            "SQL: stion : Show me the price from products . SQL : S Q L E L E C T p r i c e F R O M p r o d u c t s ; < E N D\n",
            "------------------------------\n",
            "Q: List all users where name is Alice.\n",
            "SQL: stion : List all users where name is l ice . SQL : S Q L E C T t o t a l _ a m o u n t F R O M o r d e r s ; <\n",
            "------------------------------\n",
            "Q: Count the number of employees.\n",
            "SQL: stion : Count the number of employees . SQL : S E L E C T i d F R O M p r o d u c t s W H E R E i d = 6 5\n",
            "------------------------------\n",
            "Q: Find the department for employees with id 10.\n",
            "SQL: stion : Find the department for employees with id 10 . SQL : S E L E C T s t o c k _ c o u n t F R O M p r o d u c t s W\n",
            "------------------------------\n"
          ]
        }
      ]
    }
  ]
}