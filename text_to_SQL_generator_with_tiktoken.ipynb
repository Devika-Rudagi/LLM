{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Configuration and data generation"
      ],
      "metadata": {
        "id": "a3wX74Jer624"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import tiktoken\n",
        "import random\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "batch_size = 16\n",
        "block_size = 128       # Context window\n",
        "max_iters = 5000       # Training steps\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 100\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# --- 2. DATA GENERATION (Synthetic SQL) ---\n",
        "def generate_sql_data(num_samples=5000):\n",
        "    tables = ['users', 'products', 'orders', 'employees']\n",
        "    columns = {\n",
        "        'users': ['id', 'name', 'email', 'signup_date'],\n",
        "        'products': ['id', 'title', 'price', 'stock_count'],\n",
        "        'orders': ['id', 'user_id', 'order_date', 'total_amount'],\n",
        "        'employees': ['id', 'first_name', 'department', 'salary']\n",
        "    }\n",
        "    templates = [\n",
        "        (\"Show me the {col} from {tab}.\", \"SELECT {col} FROM {tab};\"),\n",
        "        (\"List all {tab} where {col} is {val}.\", \"SELECT * FROM {tab} WHERE {col} = '{val}';\"),\n",
        "        (\"Find the {col} for {tab} with id {val}.\", \"SELECT {col} FROM {tab} WHERE id = {val};\"),\n",
        "        (\"Count the number of {tab}.\", \"SELECT count(*) FROM {tab};\")\n",
        "    ]\n",
        "\n",
        "    data = []\n",
        "    for _ in range(num_samples):\n",
        "        tab = random.choice(tables)\n",
        "        col = random.choice(columns[tab])\n",
        "        val = random.randint(1, 100)\n",
        "        tmpl_q, tmpl_s = random.choice(templates)\n",
        "\n",
        "        q = tmpl_q.format(tab=tab, col=col, val=val)\n",
        "        s = tmpl_s.format(tab=tab, col=col, val=val)\n",
        "\n",
        "        # We add a special delimiter <END> so the model knows when to stop\n",
        "        entry = f\"Question: {q}\\nSQL: {s}\\n<END>\\n\"\n",
        "        data.append(entry)\n",
        "    return \"\".join(data)\n",
        "\n",
        "raw_text = generate_sql_data()"
      ],
      "metadata": {
        "id": "jsVpXDnSfDNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# create token and model definitions"
      ],
      "metadata": {
        "id": "m21TjPgisAqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. TOKENIZATION (BPE with tiktoken) ---\n",
        "# We use the GPT-4 tokenizer ('cl100k_base') for 100K tokens and gpt2 for 50k\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Encode the entire dataset\n",
        "train_ids = enc.encode(raw_text)\n",
        "print(f\"Total tokens in dataset: {len(train_ids)}\")\n",
        "print(f\"Vocabulary size: {enc.n_vocab}\")\n",
        "\n",
        "# Convert to tensor\n",
        "data = torch.tensor(train_ids, dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Because BPE vocab is large (100k+), we need to update vocab_size\n",
        "vocab_size = enc.n_vocab\n",
        "\n",
        "# --- 4. MODEL COMPONENTS (Standard Transformer) ---\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        # Using PyTorch's optimized MultiheadAttention\n",
        "        self.sa = nn.MultiheadAttention(embed_dim=n_embd, num_heads=n_head,\n",
        "                                        dropout=dropout, batch_first=True)\n",
        "        self.ffwd = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        # 1. MANUALLY CREATE MASK\n",
        "        # Generates a (T, T) matrix of -inf (top right) and 0 (bottom left).\n",
        "        # This prevents the model from seeing future tokens.\n",
        "        attn_mask = nn.Transformer.generate_square_subsequent_mask(T).to(device)\n",
        "        x_norm = self.ln1(x)\n",
        "        # We pass x_norm three times (Query, Key, Value).\n",
        "        # We pass 'attn_mask' to force causality.\n",
        "        attn_out, _ = self.sa(x_norm, x_norm, x_norm, attn_mask=attn_mask, need_weights=False)\n",
        "        x = x + attn_out\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I9UvBzxGKwf",
        "outputId": "7552922e-3d52-43e1-d90b-8b2d9fd40034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens in dataset: 131910\n",
            "Vocabulary size: 50257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "b8XHYNpNsD0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. TRAINING LOOP ---\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in m.parameters())/1e6:.2f}M\")\n",
        "print(\"Starting training...\")\n",
        "\n",
        "for iter in range(3000): #max_iters\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step {iter}: Train Loss {losses['train']:.4f}, Val Loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAoo3QYaWSSH",
        "outputId": "44a7bd91-b5c6-49b5-c023-e9fa65846fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 49.34M\n",
            "Starting training...\n",
            "Step 0: Train Loss 10.9326, Val Loss 10.9338\n",
            "Step 500: Train Loss 0.3419, Val Loss 0.3410\n",
            "Step 1000: Train Loss 0.3306, Val Loss 0.3374\n",
            "Step 1500: Train Loss 0.2670, Val Loss 0.2873\n",
            "Step 2000: Train Loss 0.2442, Val Loss 0.2691\n",
            "Step 2500: Train Loss 0.2269, Val Loss 0.2694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "qfoigR2esFzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 6. INFERENCE (Text-to-SQL) ---\n",
        "print(\"\\n--- INFERENCE TEST ---\")\n",
        "\n",
        "def generate_sql_bpe(question):\n",
        "    prompt = f\"Question: {question}\\nSQL:\"\n",
        "    # Encode with BPE\n",
        "    input_ids = enc.encode(prompt)\n",
        "    context = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    # Generate\n",
        "    output_ids = m.generate(context, max_new_tokens=50)[0].tolist()\n",
        "    # Decode with BPE\n",
        "    output_text = enc.decode(output_ids)\n",
        "\n",
        "    # Parse output\n",
        "    try:\n",
        "        start_marker = \"SQL:\"\n",
        "        end_marker = \"<END>\"\n",
        "\n",
        "        # We start searching AFTER the prompt to avoid finding the prompt's own \"SQL:\"\n",
        "        start_idx = output_text.find(start_marker) + len(start_marker)\n",
        "\n",
        "        # Extract everything after \"SQL:\"\n",
        "        generated_sql = output_text[start_idx:]\n",
        "\n",
        "        # Stop at <END> or newline if <END> isn't generated\n",
        "        if end_marker in generated_sql:\n",
        "            generated_sql = generated_sql.split(end_marker)[0]\n",
        "        else:\n",
        "             generated_sql = generated_sql.split('\\n')[0]\n",
        "\n",
        "        return generated_sql.strip()\n",
        "    except:\n",
        "        return output_text\n",
        "\n",
        "# Test Questions\n",
        "test_qs = [\n",
        "    \"Show me the price from products.\",\n",
        "    \"List all users where name is Alice.\",\n",
        "    \"Count the number of employees.\",\n",
        "    \"Find the department for employees with id 10.\"\n",
        "]\n",
        "\n",
        "for q in test_qs:\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"SQL: {generate_sql_bpe(q)}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCm0-DfgfWht",
        "outputId": "412cfc9d-6e21-4f5a-a5a1-a40f123e3b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- INFERENCE TEST ---\n",
            "Q: Show me the price from products.\n",
            "SQL: SELECT price FROM products;\n",
            "------------------------------\n",
            "Q: List all users where name is Alice.\n",
            "SQL: SELECT * FROM users WHERE name = '29';\n",
            "------------------------------\n",
            "Q: Count the number of employees.\n",
            "SQL: SELECT count(*) FROM employees;\n",
            "------------------------------\n",
            "Q: Find the department for employees with id 10.\n",
            "SQL: SELECT department FROM employees WHERE id = 10;\n",
            "------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}